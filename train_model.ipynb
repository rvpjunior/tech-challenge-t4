{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constants\n",
    "DATA_FILE = \"data_GOOGL_2010-2024.csv\"\n",
    "SEQUENCE_INTERVAL = 15\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.2\n",
    "LSTM_UNITS = 50\n",
    "DENSE_UNITS = 50\n",
    "MIN_LEARNING_RATE = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "REDUCE_LR_PATIENCE = 10\n",
    "REDUCE_LR_FACTOR = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled = scaler.fit_transform(df[['Close', 'High', 'Low', 'Open', 'Volume']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into X and y\n",
    "X, y = [], []\n",
    "for i in range(len(df_scaled) - SEQUENCE_INTERVAL):\n",
    "    X.append(df_scaled[i:i+SEQUENCE_INTERVAL])\n",
    "    y.append(df_scaled[i+SEQUENCE_INTERVAL, 0])\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(X_train.flatten(), bins=50, alpha=0.5, label=\"Treino\")\n",
    "plt.hist(X_test.flatten(), bins=50, alpha=0.5, label=\"Validação\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribuição dos Dados\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(units=LSTM_UNITS, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(DROPOUT),\n",
    "    LSTM(units=LSTM_UNITS, return_sequences=True),\n",
    "    Dropout(DROPOUT),\n",
    "    LSTM(units=LSTM_UNITS, return_sequences=False),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(units=DENSE_UNITS),\n",
    "    Dense(units=1)\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=REDUCE_LR_FACTOR, patience=EARLY_STOPPING_PATIENCE, min_lr=MIN_LEARNING_RATE)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=REDUCE_LR_PATIENCE, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"Loss Treino\")\n",
    "plt.plot(history.history['val_loss'], label=\"Loss Validação\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = scaler.inverse_transform(np.column_stack((y_pred, np.zeros((y_pred.shape[0], 4)))))[:, 0]\n",
    "y_test_real = scaler.inverse_transform(np.column_stack((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], 4)))))[:, 0]\n",
    "\n",
    "# Calculating the difference between real and predicted values\n",
    "difference = y_test_real - y_pred\n",
    "\n",
    "# Displaying the difference for each entry\n",
    "for i in range(len(difference)):\n",
    "    print(f\"Entry {i+1}: Real = {y_test_real[i]}, Predicted = {y_pred[i]}, Difference = {difference[i]}\")\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred))\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"google_prediction_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
